# app/main.py - ULTIMATE RAG System for Perfect Accuracy
import os
import re
import time
import logging
import uuid
import json
import asyncio
import io
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime

# FastAPI imports
from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# Core imports
import aiohttp
import PyPDF2
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Environment configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
API_KEY = os.getenv("API_KEY", "6d2683f80eca9847d20948e1e5508885d08fdc65d943182f85de250687859193")

# Pydantic models
class CompetitionRequest(BaseModel):
    documents: str
    questions: List[str]
    session_id: Optional[str] = None

class CompetitionResponse(BaseModel):
    answers: List[str]
    session_id: str

class UltimateRAGSystem:
    """Ultimate RAG system designed for perfect accuracy and speed."""

    def __init__(self):
        logger.info("üöÄ Initializing ULTIMATE RAG System for Perfect Accuracy")
        self.openai_embeddings = None
        self.llm = None
        self.session = None
        self.document_store = {} # Using a dictionary to store session-based data

        # Optimized text splitter for insurance documents
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Slightly larger chunks for better context
            chunk_overlap=250, # High overlap to prevent losing info
            length_function=len,
            separators=["\n\n", "\n", ". ", ";", ":", ",", " "],
            add_start_index=True,
        )
        
        self._initialize_ultimate_openai()
        logger.info("‚úÖ ULTIMATE RAG System ready for perfect accuracy")

    def _initialize_ultimate_openai(self):
        """Initialize OpenAI with optimal settings for accuracy and speed."""
        try:
            if not OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY is required!")
            
            self.openai_embeddings = OpenAIEmbeddings(
                api_key=OPENAI_API_KEY,
                model="text-embedding-3-small",
                show_progress_bar=False,
                chunk_size=1000
            )
            
            self.llm = ChatOpenAI(
                api_key=OPENAI_API_KEY,
                model="gpt-4o-mini",
                temperature=0, # For deterministic and factual answers
                max_tokens=500, 
                timeout=15 # Increased timeout for complex synthesis
            )
            logger.info("‚úÖ OpenAI GPT-4o-mini and Embeddings ready - MAXIMUM SPEED mode")

        except Exception as e:
            logger.error(f"‚ùå OpenAI initialization failed: {str(e)}")
            raise

    async def get_session(self):
        """Get or create an aiohttp ClientSession."""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session

    async def process_document_ultimate(self, document_url: str, session_id: str) -> None:
        """Process a document and prepare it for Q&A, storing it by session_id."""
        process_start = time.time()
        logger.info(f"[{session_id}] üìÑ Ultimate processing started for: {document_url[:80]}...")

        try:
            content = await self._fetch_content_ultimate(document_url, session_id)
            
            if not content or len(content.strip()) < 50:
                raise ValueError("Document content is insufficient for processing.")
            
            doc = Document(page_content=content, metadata={"source": document_url})
            chunks = self.text_splitter.split_documents([doc])
            
            if not chunks:
                raise ValueError("Failed to create chunks from the document.")

            logger.info(f"[{session_id}] üìÑ Created {len(chunks)} optimized chunks")
            
            faiss_store = FAISS.from_documents(chunks, self.openai_embeddings)
            
            self.document_store[session_id] = {
                "chunks": chunks,
                "faiss_store": faiss_store
            }

            process_time = time.time() - process_start
            logger.info(f"[{session_id}] ‚úÖ Ultimate document processing complete in {process_time:.2f}s")

        except Exception as e:
            logger.error(f"[{session_id}] ‚ùå Ultimate document processing failed: {str(e)}")
            raise

    async def _fetch_content_ultimate(self, url: str, session_id: str) -> str:
        """Fetch and extract content from a URL with high efficiency."""
        try:
            session = await self.get_session()
            async with session.get(url) as response:
                response.raise_for_status()
                content_data = await response.read()
                
                if url.lower().endswith('.pdf'):
                    return self._extract_pdf_ultimate(content_data, session_id)
                else:
                    return content_data.decode('utf-8', errors='ignore')
        except Exception as e:
            logger.error(f"[{session_id}] ‚ùå Content fetch failed for {url}: {str(e)}")
            raise

    def _extract_pdf_ultimate(self, pdf_data: bytes, session_id: str) -> str:
        """Extract text from a PDF with a focus on preserving structure and fixing broken lines."""
        extract_start = time.time()
        try:
            pdf_file = io.BytesIO(pdf_data)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            full_text = []
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    text = page.extract_text()
                    if text:
                        full_text.append(text)
                except Exception as page_exc:
                    logger.warning(f"[{session_id}] Could not extract text from page {page_num + 1}: {page_exc}")
            
            if not full_text:
                raise ValueError("No readable content extracted from PDF.")
            
            # Combine text and perform cleaning
            combined_text = "\n".join(full_text)
            # Attempt to fix lines broken mid-sentence
            combined_text = re.sub(r'([a-z])\n([a-z])', r'\1 \2', combined_text)
            combined_text = self._clean_text_ultimate(combined_text)

            extract_time = time.time() - extract_start
            logger.info(f"[{session_id}] üìã Ultimate PDF extraction: {extract_time:.2f}s, {len(combined_text)} chars")
            return combined_text

        except Exception as e:
            logger.error(f"[{session_id}] ‚ùå Ultimate PDF extraction failed: {str(e)}")
            raise

    def _clean_text_ultimate(self, text: str) -> str:
        """Clean text while preserving important formatting for insurance policies."""
        text = re.sub(r'\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        return text.strip()

    async def answer_questions_ultimate(self, questions: List[str], session_id: str) -> List[str]:
        """Answer a list of questions in parallel for maximum speed."""
        qa_start = time.time()
        logger.info(f"[{session_id}] üéØ Answering {len(questions)} questions with ultimate accuracy...")

        tasks = [self._answer_single_question(q, session_id, i + 1) for i, q in enumerate(questions)]
        answers = await asyncio.gather(*tasks)

        total_time = time.time() - qa_start
        logger.info(f"[{session_id}] üéØ All questions answered in {total_time:.2f}s")
        return answers

    async def _answer_single_question(self, question: str, session_id: str, q_num: int) -> str:
        """Handle the full RAG process for a single question."""
        q_start = time.time()
        try:
            logger.info(f"[{session_id}] ‚ùì Q{q_num}: Processing '{question[:60]}...'")
            
            if session_id not in self.document_store:
                return "Error: Document not processed for this session. Please submit the document first."

            relevant_contexts = self._get_ultimate_contexts(question, session_id)
            answer = await self._generate_ultimate_answer(question, relevant_contexts, session_id, q_num)
            
            q_time = time.time() - q_start
            logger.info(f"[{session_id}] ‚úÖ Q{q_num} completed in {q_time:.2f}s")
            return answer

        except Exception as e:
            logger.error(f"[{session_id}] ‚ùå Q{q_num} failed: {str(e)}")
            return "An error occurred while processing this question."

    def _get_ultimate_contexts(self, question: str, session_id: str) -> List[str]:
        """Get the most relevant contexts using a multi-layered retrieval strategy."""
        session_data = self.document_store[session_id]
        faiss_store = session_data["faiss_store"]
        all_chunks = session_data["chunks"]
        
        # Layer 1: High-Precision Semantic Search
        try:
            semantic_results_with_scores = faiss_store.similarity_search_with_score(question, k=5)
            # FAISS returns L2 distance, lower is better. Filter out irrelevant results.
            semantic_results = [doc for doc, score in semantic_results_with_scores if score < 1.0]
        except Exception:
            semantic_results = faiss_store.similarity_search(question, k=5)


        # Layer 2: Broad Keyword Search for specific terms
        keywords = re.findall(r'\b\w{3,}\b', question.lower())
        keyword_results = []
        for chunk in all_chunks:
            text_lower = chunk.page_content.lower()
            if any(key in text_lower for key in keywords):
                keyword_results.append(chunk)

        # Layer 3: Re-ranking and Merging
        # Boost score of chunks that appear in both semantic and keyword searches
        # and prioritize chunks with numbers, which are crucial for policy details.
        final_candidates = {}
        for doc in semantic_results:
            score = 1.5 + (1 if any(char.isdigit() for char in doc.page_content) else 0)
            final_candidates[doc.page_content] = score
        
        for doc in keyword_results:
            score = final_candidates.get(doc.page_content, 0) + 1.0
            score += (1 if any(char.isdigit() for char in doc.page_content) else 0)
            final_candidates[doc.page_content] = score

        # Sort by score and return the top contexts
        sorted_contexts = sorted(final_candidates.keys(), key=lambda x: final_candidates[x], reverse=True)
        
        logger.info(f"[{session_id}] üîç Multi-layered search found {len(sorted_contexts)} relevant contexts.")
        return sorted_contexts[:8] # Return top 8 for rich context

    async def _generate_ultimate_answer(self, question: str, contexts: List[str], session_id: str, q_num: int) -> str:
        """Generate a precise answer using a structured, few-shot, fact-checking prompt."""
        if not contexts:
            return "The requested information could not be located in the provided document."
        
        context_text = "\n\n".join([f"--- CONTEXT ---\n{ctx}" for ctx in contexts])
        
        # This is a highly structured prompt designed to force the LLM to be a fact-checker
        structured_prompt = f"""
You are a meticulous AI Insurance Analyst. Your task is to provide a precise, factual answer to the user's question based exclusively on the provided policy document context.

**POLICY DOCUMENT CONTEXT:**
{context_text}

**USER'S QUESTION:**
"{question}"

**MANDATORY INSTRUCTIONS:**
1.  **Fact Extraction:** Scrutinize the context to find all specific facts, numbers (e.g., 30, 36, 24), percentages (e.g., 1%, 2%, 5%), and conditions related to the question.
2.  **Answer Synthesis:** Construct a single, professional, and complete sentence that directly answers the question using ONLY the extracted facts.
3.  **Formatting:** The answer MUST match the style of the "Sample Answer Format" below. Do not add any conversational filler. Start the answer directly.
4.  **CRITICAL RULE:** If you find any relevant numbers or policy terms in the context, you are FORBIDDEN from stating the information is unavailable. You MUST synthesize an answer.

**SAMPLE ANSWER FORMAT (FOR STYLE AND TONE):**
"A grace period of thirty days is provided for premium payment after the due date to renew or continue the policy without losing continuity benefits."
"There is a waiting period of thirty-six (36) months of continuous coverage from the first policy inception for pre-existing diseases and their direct complications to be covered."
"Yes, the policy covers maternity expenses, including childbirth and lawful medical termination of pregnancy. To be eligible, the female insured person must have been continuously covered for at least 24 months. The benefit is limited to two deliveries or terminations during the policy period."

**YOUR PRECISE, FACT-BASED ANSWER:**
"""

        try:
            response = await asyncio.to_thread(
                self.llm.invoke,
                structured_prompt
            )
            answer = response.content.strip()

            # Final cleanup to ensure format compliance
            if answer and not answer.endswith('.'):
                answer += '.'
            
            return answer or "The specific details for this question were not found in the provided context."
        except Exception as e:
            logger.error(f"[{session_id}] ‚ùå LLM generation for Q{q_num} failed: {str(e)}")
            return "The AI model could not generate an answer due to an internal error."

    async def process_request_ultimate(self, documents: str, questions: List[str], session_id: Optional[str] = None) -> Tuple[List[str], str]:
        """Main entry point to process a full competition request."""
        total_start = time.time()
        
        if not session_id:
            session_id = f"ultimate_session_{uuid.uuid4().hex[:8]}"
        
        logger.info(f"üöÄ Starting ULTIMATE request for session: {session_id}")
        
        try:
            if session_id not in self.document_store:
                await self.process_document_ultimate(documents, session_id)
            else:
                logger.info(f"[{session_id}] Document already processed. Skipping ingestion.")
            
            answers = await self.answer_questions_ultimate(questions, session_id)
            
            total_time = time.time() - total_start
            logger.info(f"‚úÖ ULTIMATE request {session_id} completed in {total_time:.2f}s")
            
            return answers, session_id
        
        except Exception as e:
            total_time = time.time() - total_start
            logger.error(f"‚ùå ULTIMATE request {session_id} failed after {total_time:.2f}s: {str(e)}")
            raise

    async def cleanup(self):
        """Cleanup resources like the aiohttp session."""
        if self.session:
            await self.session.close()
            logger.info("HTTP session closed.")

# Initialize ultimate system
ultimate_rag = UltimateRAGSystem()

# FastAPI Application
app = FastAPI(
    title="HackRx 6.0 - ULTIMATE RAG System",
    description="An optimized RAG system for perfect accuracy and high-speed responses.",
    version="8.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

security = HTTPBearer(auto_error=False)

async def verify_token(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)):
    """Verify the API token from the Authorization header."""
    if not credentials or credentials.credentials != API_KEY:
        logger.warning("Authentication failed: Invalid or missing token.")
        raise HTTPException(status_code=401, detail="Invalid or missing authentication token")
    return credentials.credentials

@app.get("/")
async def root():
    return {
        "message": "HackRx 6.0 - ULTIMATE RAG System",
        "status": "ready",
        "version": "8.0.0",
        "accuracy_mode": "ULTIMATE",
        "tech_stack": {
            "embeddings": "‚úÖ OpenAI text-embedding-3-small",
            "vector_store": "‚úÖ FAISS",
            "llm": "‚úÖ OpenAI GPT-4o-mini",
            "search_strategy": "‚úÖ Multi-Layered Retrieval & Re-ranking",
            "prompting": "‚úÖ Structured Fact-Checking Synthesis"
        },
        "performance_targets": {
            "accuracy": "Perfect (like sample response)",
            "response_time": "< 16 seconds",
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "openai_llm_ready": bool(ultimate_rag.llm),
        "openai_embeddings_ready": bool(ultimate_rag.openai_embeddings),
    }

@app.post("/hackrx/run", response_model=CompetitionResponse)
async def ultimate_competition_endpoint(
    request: CompetitionRequest,
    token: str = Depends(verify_token)
):
    """ULTIMATE competition endpoint for perfect accuracy and speed."""
    request_id = str(uuid.uuid4())[:8]
    request_start = time.time()
    
    logger.info(f"üöÄ Received ULTIMATE Request {request_id}")
    
    try:
        if not request.documents:
            raise HTTPException(status_code=400, detail="Missing 'documents' URL")
        if not request.questions:
            raise HTTPException(status_code=400, detail="Missing 'questions' list")
        
        answers, session_id = await ultimate_rag.process_request_ultimate(
            request.documents, 
            request.questions, 
            request.session_id
        )
        
        if len(answers) != len(request.questions):
            raise HTTPException(status_code=500, detail="Mismatch in number of answers generated.")
        
        request_time = time.time() - request_start
        logger.info(f"‚úÖ ULTIMATE Request {request_id} successfully processed in {request_time:.2f}s")
        
        return CompetitionResponse(answers=answers, session_id=session_id)
        
    except HTTPException:
        raise
    except Exception as e:
        request_time = time.time() - request_start
        logger.error(f"‚ùå ULTIMATE Request {request_id} failed after {request_time:.2f}s: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected server error occurred: {str(e)}")

@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ ULTIMATE RAG System Starting Up...")
    await ultimate_rag.get_session()
    logger.info("‚úÖ System startup complete. Ready for requests.")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("üîÑ Shutting down ULTIMATE system...")
    await ultimate_rag.cleanup()
    logger.info("‚úÖ Shutdown complete.")
